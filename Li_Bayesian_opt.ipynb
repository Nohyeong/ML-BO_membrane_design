{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nohyeong/ML-BO_membrane_design/blob/main/Li_Bayesian_opt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwWJFB4u7_kx",
        "outputId": "b6531f14-fae1-441f-b9d5-b529e2b7efa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2023.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2023.9.6\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.11.4)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (2.0.3)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.5.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.0)\n",
            "Installing collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.6.3\n",
            "Collecting shap\n",
            "  Downloading shap-0.45.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.5/540.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.0.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.4)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.0)\n",
            "Collecting slicer==0.0.8 (from shap)\n",
            "  Downloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.45.1 slicer-0.0.8\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "!pip install rdkit\n",
        "!pip install category_encoders\n",
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcLlaCIj8GWI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import category_encoders as ce\n",
        "from xgboost import XGBRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from hyperopt import hp, tpe, Trials, STATUS_OK, fmin\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, PowerTransformer, QuantileTransformer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "sc = PowerTransformer()\n",
        "\n",
        "def createFolder(directory):\n",
        "    try:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "    except OSError:\n",
        "        print('Error: Creating directory. ' + directory)\n",
        "\n",
        "class morgan_fp:\n",
        "    def __init__(self, radius, length):\n",
        "        self.radius = radius\n",
        "        self.length = length\n",
        "    def __call__(self, smiles):\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, int(self.radius), int(self.length))\n",
        "        npfp = np.array(list(fp.ToBitString())).astype('float32')\n",
        "        return npfp\n",
        "\n",
        "def conv_data_pd(data, data_t, fp, en, scaler, numeric_features, salt_per):\n",
        "    data['a1-fp'] = data['a1-smile'].apply(fp)\n",
        "    data['a2-fp'] = data['a2-smile'].apply(fp)\n",
        "\n",
        "    data['a-fp'] = [x + y for x, y in zip(data['a1-fp'], data['a2-fp'])]\n",
        "    data['b-fp'] = data['b-smile'].apply(fp)\n",
        "\n",
        "    x_a=np.array(list(data['a-fp']))\n",
        "    x_b=np.array(list(data['b-fp']))\n",
        "\n",
        "    if salt_per == 'per':\n",
        "      X_train = data_t.drop(columns=['Monomer A1 type', 'a1-smile','Monomer A2 type', 'a2-smile', 'b-smile', 'a1-fp', 'a2-fp', 'a-fp','b-fp']).copy()\n",
        "\n",
        "    elif salt_per == 'rej':\n",
        "      X_train = data_t.drop(columns=['Monomer A1 type', 'a1-smile','Monomer A2 type', 'a2-smile', 'b-smile', 'a1-fp', 'a2-fp', 'a-fp','b-fp']).copy()\n",
        "\n",
        "    if salt_per == 'per':\n",
        "      x = data.drop(columns=['Monomer A1 type', 'a1-smile','Monomer A2 type', 'a2-smile', 'b-smile', 'a1-fp', 'a2-fp', 'a-fp','b-fp']).copy()\n",
        "\n",
        "    elif salt_per == 'rej':\n",
        "      x = data.drop(columns=['Monomer A1 type', 'a1-smile','Monomer A2 type', 'a2-smile', 'b-smile', 'a1-fp', 'a2-fp', 'a-fp','b-fp']).copy()\n",
        "\n",
        "    hh=en.fit_transform(X_train[categorical_features])\n",
        "    # SC= scaler.fit(hh[numeric_features])\n",
        "    columns_to_drop = ['a1-fp', 'a2-fp', 'a-fp', 'b-fp']\n",
        "    x.drop(columns=[col for col in columns_to_drop if col in x.columns], inplace=True)\n",
        "\n",
        "    col_pd = en.transform(x[categorical_features])\n",
        "    xxxx = x[numeric_features]\n",
        "    num_pd = pd.DataFrame(data= xxxx, columns=numeric_features) #pd\n",
        "    xxxxx = np.concatenate((x_a, x_b), axis =1)\n",
        "    fp_pd = pd.DataFrame(data= xxxxx, columns=[f'f_{i}' for i in range(2*x_a.shape[1])])\n",
        "\n",
        "    x_pd = pd.concat([fp_pd, col_pd, num_pd], axis=1)\n",
        "    x_pd.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return x_pd\n",
        "\n",
        "def conv_data_pd_opt(data, data_t, fp, en, scaler, numeric_features, salt_per):\n",
        "\n",
        "    data['a1-fp'] = data['a1-smile'].apply(fp)\n",
        "    data['a2-fp'] = data['a2-smile'].apply(fp)\n",
        "\n",
        "    data['a-fp'] = [x + y for x, y in zip(data['a1-fp'], data['a2-fp'])]\n",
        "    data['b-fp'] = data['b-smile'].apply(fp)\n",
        "\n",
        "    x_a=np.array(list(data['a-fp']))\n",
        "    x_b=np.array(list(data['b-fp']))\n",
        "\n",
        "    if salt_per == 'per':\n",
        "      X_train = data_t.drop(columns=['Monomer A1 type', 'a1-smile','Monomer A2 type', 'a2-smile', 'b-smile']).copy()\n",
        "\n",
        "    elif salt_per == 'rej':\n",
        "      X_train = data_t.drop(columns=['Monomer A1 type', 'a1-smile','Monomer A2 type', 'a2-smile', 'b-smile']).copy()\n",
        "\n",
        "    if salt_per == 'per':\n",
        "      x = data.drop(columns=['Monomer A1 type', 'a1-smile','Monomer A2 type', 'a2-smile', 'b-smile']).copy()\n",
        "\n",
        "    elif salt_per == 'rej':\n",
        "      x = data.drop(columns=['Monomer A1 type', 'a1-smile','Monomer A2 type', 'a2-smile', 'b-smile']).copy()\n",
        "\n",
        "    hh=en.fit_transform(X_train[categorical_features])\n",
        "    # SC= scaler.fit(hh[numeric_features])\n",
        "    columns_to_drop = ['a1-fp', 'a2-fp', 'a-fp', 'b-fp']\n",
        "    x.drop(columns=[col for col in columns_to_drop if col in x.columns], inplace=True)\n",
        "\n",
        "    col_pd = en.transform(x[categorical_features])\n",
        "    xxxx = x[numeric_features]\n",
        "    num_pd = pd.DataFrame(data= xxxx, columns=numeric_features) #pd\n",
        "    xxxxx = np.concatenate((x_a, x_b), axis =1)\n",
        "    fp_pd = pd.DataFrame(data= xxxxx, columns=[f'f_{i}' for i in range(2*x_a.shape[1])])\n",
        "\n",
        "    x_pd = pd.concat([fp_pd, col_pd, num_pd], axis=1)\n",
        "    x_pd.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return x_pd\n",
        "\n",
        "\n",
        "def objective_per(params):\n",
        "    global ITERATION\n",
        "    ITERATION +=1\n",
        "    for name in ['max_depth', 'n_estimators','fp_length']:\n",
        "        params[name] = int(params[name])\n",
        "    loss, train_loss = fit(params, train_data_per, numeric_features, 'per')\n",
        "    loss =loss\n",
        "    off_connection = open(out_file, 'a')\n",
        "    writer = csv.writer(off_connection)\n",
        "    writer.writerow([loss,train_loss, params, ITERATION])\n",
        "    pickle.dump(bayes_trial, open(\"per_2_xgb.p\", \"wb\"))\n",
        "    return {'loss':loss,'train_loss':train_loss, 'params': params, 'iteration':ITERATION, 'status':STATUS_OK}\n",
        "\n",
        "def objective_rej(params):\n",
        "    global ITERATION\n",
        "    ITERATION +=1\n",
        "    for name in ['max_depth', 'n_estimators','fp_length']:\n",
        "        params[name] = int(params[name])\n",
        "    loss, train_loss = fit(params, train_data_rej, numeric_features, 'rej')\n",
        "    loss =loss\n",
        "    off_connection = open(out_file, 'a')\n",
        "    writer = csv.writer(off_connection)\n",
        "    writer.writerow([loss,train_loss, params, ITERATION])\n",
        "    pickle.dump(bayes_trial, open(\"rej_1.p\", \"wb\"))\n",
        "    return {'loss':loss,'train_loss':train_loss, 'params': params, 'iteration':ITERATION, 'status':STATUS_OK}\n",
        "\n",
        "def data_split(train_data_per, n_splits, test_size, random_state):\n",
        "  sss = ShuffleSplit(n_splits=1, test_size=0.2,random_state=10)\n",
        "\n",
        "  for train_index, test_index in sss.split(train_data_per):\n",
        "      train_data = train_data_per.iloc[train_index]\n",
        "      test = train_data_per.iloc[test_index]\n",
        "      train_data.reset_index(drop=True, inplace=True)\n",
        "      test.reset_index(drop=True, inplace=True)\n",
        "      # train_data.to_csv('per_train_data.csv', index = False)\n",
        "      # test.to_csv('per_test.csv', index = False)\n",
        "  return train_data, test\n",
        "\n",
        "fp_length_list = [2048]\n",
        "\n",
        "space = {\n",
        "         'max_delta_step': hp.uniform('max_delta_step', 0, 10),\n",
        "         'learning_rate': hp.uniform('learning_rate', 0.001, 0.1),\n",
        "         'max_depth': hp.quniform('max_depth', 1,10,1),\n",
        "         'min_child_weight': hp.uniform('min_child_weight', 1,10),\n",
        "         'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
        "         'reg_alpha':hp.uniform('reg_alpha', 0, 10),\n",
        "         'gamma':hp.uniform('gamma', 0, 10),\n",
        "         'reg_lambda':hp.uniform('reg_lambda', 0, 10),\n",
        "         'n_estimators': hp.quniform('n_estimators', 1, 1000, 1),\n",
        "         'fp_length': hp.choice('fp_length', fp_length_list)\n",
        "}\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.metrics import r2_score\n",
        "from scipy.stats import linregress\n",
        "\n",
        "def make_plot(total_data, range, tick):\n",
        "\n",
        "  X_title = 'Real removal (%)'\n",
        "  Y_title = 'Pred removal (%)'\n",
        "\n",
        "  title_font_size = 43 # 33\n",
        "  tickfont_size = 35 # 23\n",
        "  ticklen = 5 # 5\n",
        "  tickwidth = 2 # 2\n",
        "  title_standoff = 20\n",
        "\n",
        "  plot_width = 610 # 700\n",
        "  plot_height = 500 # 500\n",
        "\n",
        "  fig = px.scatter(total_data, x='y_true', y='y_pred', color = 'mark',\n",
        "                  width=plot_width, height=plot_height, range_x=[-5, range], range_y=[-5, range]\n",
        "                  )\n",
        "\n",
        "  fig.update_layout(\n",
        "      xaxis = dict(\n",
        "          dtick=tick,\n",
        "          title_text = X_title,\n",
        "          title_font = {\"size\": title_font_size},\n",
        "          tickfont = dict(family='Arial', color='black', size=tickfont_size),\n",
        "          title_standoff = title_standoff,\n",
        "          color='black',\n",
        "          ticks=\"inside\",\n",
        "          ticklen=ticklen,\n",
        "          tickwidth=tickwidth,\n",
        "          title_font_family=\"Arial\"),\n",
        "      yaxis = dict(\n",
        "          dtick = tick,\n",
        "          title_text = Y_title,\n",
        "          title_font = {\"size\": title_font_size},\n",
        "          tickfont = dict(family='Arial', color='black', size=tickfont_size),\n",
        "          title_standoff = title_standoff,\n",
        "          color='black',\n",
        "          ticks=\"inside\",\n",
        "          ticklen=ticklen,\n",
        "          tickwidth=tickwidth,\n",
        "          title_font_family=\"Arial\"))\n",
        "\n",
        "  fig.update_layout(plot_bgcolor='rgb(256,256,256)', showlegend=True)\n",
        "\n",
        "  fig.update_xaxes(showline=True, linewidth=4, linecolor='black', mirror=True)\n",
        "  fig.update_yaxes(showline=True, linewidth=4, linecolor='black', mirror=True)\n",
        "\n",
        "  random_x = [0, 100]\n",
        "  random_y0 = [0, 100]\n",
        "\n",
        "  fig.add_trace(go.Scatter(x=random_x, y=random_y0, line=dict(color='red', width=2), marker=dict(size=1)))\n",
        "\n",
        "  fig.update_traces(marker=dict(size=10, opacity=0.65, line=dict(width=0.7)), selector=dict(mode='markers'))\n",
        "\n",
        "  fig.show()\n",
        "\n",
        "def hyp_opt(objective):\n",
        "  fp_length_list = [2048]\n",
        "\n",
        "  space = {\n",
        "          'max_delta_step': hp.uniform('max_delta_step', 0, 10),\n",
        "          'learning_rate': hp.uniform('learning_rate', 0.001, 0.03),\n",
        "          'max_depth': hp.quniform('max_depth', 1,10,1),\n",
        "          'min_child_weight': hp.uniform('min_child_weight', 1,10),\n",
        "          'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
        "          'reg_alpha':hp.uniform('reg_alpha', 0, 10),\n",
        "          'gamma':hp.uniform('gamma', 0, 10),\n",
        "          'reg_lambda':hp.uniform('reg_lambda', 0, 10),\n",
        "          'n_estimators': hp.quniform('n_estimators', 1, 500, 1),\n",
        "          'fp_length': hp.choice('fp_length', fp_length_list)\n",
        "  }\n",
        "\n",
        "  out_file ='per.csv'\n",
        "  off_connection =open(out_file, 'w')\n",
        "  writer = csv.writer(off_connection)\n",
        "  writer.writerow(['loss','train_loss', 'params', 'iteration'])\n",
        "  off_connection.close()\n",
        "  tpe_algo = tpe.suggest\n",
        "  bayes_trial = Trials()\n",
        "\n",
        "  global ITERATION\n",
        "  ITERATION = 0\n",
        "\n",
        "  tpe_algo = tpe.suggest\n",
        "  bayes_trial = Trials()\n",
        "\n",
        "  best = fmin(fn=objective, space=space, algo=tpe_algo, trials=bayes_trial, max_evals=50, rstate=np.random.default_rng(np.random.randint(0,50)))\n",
        "  best['fp_length'] = int(fp_length_list[best['fp_length']])\n",
        "\n",
        "  return best"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "now_utc = datetime.now(pytz.utc)\n",
        "est_timezone = pytz.timezone('US/Eastern')\n",
        "now_est = now_utc.astimezone(est_timezone)\n",
        "date = now_est.strftime('%Y%m%d_%H:%M:%S')\n",
        "\n",
        "title_rej = 'rej_prediction_XGBoost'\n",
        "\n",
        "directory_rej = '/gdrive/My Drive/ML-BO_followup/prediction_result/' + date + '_' + title_rej + '/'\n",
        "createFolder(directory_rej)\n",
        "\n",
        "columns_removed = ['Pure_water_flux, LMH/bar', 'DOI', 'cation','Additive X2 type in aqueous phase',\n",
        "                   'Additive X2 concentration, wt%', 'Transmembrane pressure, bar', 'anion', 'Aqueous phase pH']\n",
        "\n",
        "\n",
        "df = pd.read_excel('/gdrive/My Drive/Li-pore_size/ML-BO_data_Li.xlsx', sheet_name='Datasets')\n",
        "df.drop(columns_removed, axis=1, inplace=True)\n",
        "\n",
        "df.columns = ['Monomer A1 type', 'a1-smile', 'Monomer A2 type', 'a2-smile',\n",
        "              'A1/A2 ratio', 'Monomer A concentration', 'Monomer B type',\n",
        "              'b-smile', 'Monomer B concentration', 'Organic solvent type',\n",
        "              'Additive X1 in aqueous', 'Additive X1 concentration',\n",
        "               'Additive Y type in organic  phase',\n",
        "              'Additive Y concentration', 'Heat curing time',\n",
        "              'Heat curing termperature',\n",
        "              'salt_concentration', 'cat_val', 'cat_r', 'cat_hyd_r', 'cat_hyd_e',\n",
        "              'an_val', 'an_r', 'an_hyd_r', 'an_hyd_e', 'Polymerization time',\n",
        "              'Permeability', 'salt_rejection', 'Done']\n",
        "\n",
        "categorical_features = ['Organic solvent type', 'Additive X1 in aqueous']\n",
        "\n",
        "numeric_features = ['A1/A2 ratio', 'Monomer A concentration', 'Monomer B concentration', 'Additive X1 concentration',\n",
        "                    'Additive Y concentration', 'Heat curing time', 'Heat curing termperature',\n",
        "                    'salt_concentration', 'cat_val', 'cat_r', 'cat_hyd_r', 'cat_hyd_e', 'an_val', 'an_r', 'an_hyd_r',\n",
        "                    'an_hyd_e', 'Polymerization time']\n",
        "\n",
        "df[numeric_features] = df[numeric_features].replace('-', np.nan)\n",
        "\n",
        "df['a2-smile'] =  df['a2-smile'].replace('-', 'C')\n",
        "\n",
        "df['Monomer A concentration'] = pd.to_numeric(df['Monomer A concentration'], errors='coerce')\n",
        "df['salt_rejection'] = pd.to_numeric(df['salt_rejection'], errors='coerce')\n",
        "\n",
        "df = df[df['a1-smile'] != 'xxx']\n",
        "df = df[df['a2-smile'] != 'xxx']\n",
        "df = df[df['b-smile'] != 'xxx']\n",
        "df = df[df['Monomer A1 type'] != 'Potassium 2,5-dihydroxybenzenesulfonate']\n",
        "\n",
        "df['a1-smile'] = df['a1-smile'].fillna('C')\n",
        "df['a2-smile'] = df['a2-smile'].fillna('C')\n",
        "df['b-smile'] = df['b-smile'].fillna('C')\n",
        "\n",
        "total_df = df[df['Done']==1]\n",
        "total_df.reset_index(drop=True, inplace=True)\n",
        "total_df.drop(columns=['Done', 'Monomer B type'], inplace=True)\n",
        "\n",
        "total_df['salt_rejection'] = total_df['salt_rejection'].replace('-', np.nan)\n",
        "total_df['Permeability'] = total_df['Permeability'].replace('-', np.nan)\n",
        "total_df['a2-smile'] = total_df['a2-smile'].replace('-', np.nan)\n",
        "\n",
        "total_rej = total_df.dropna(subset=['salt_rejection'])\n",
        "y_rej = total_rej['salt_rejection']\n",
        "total_rej = total_rej.drop(columns=['Permeability', 'salt_rejection'])\n",
        "\n",
        "total_per = total_df.dropna(subset=['Permeability'])\n",
        "y_per = total_per['Permeability']\n",
        "total_per = total_per.drop(columns=['Permeability', 'salt_rejection'])\n",
        "\n",
        "X_train_rej, X_test_rej, y_train_rej, y_test_rej = train_test_split(total_rej, y_rej, test_size=0.2, random_state=42)\n",
        "X_train_rej.reset_index(drop=True, inplace=True)\n",
        "X_test_rej.reset_index(drop=True, inplace=True)\n",
        "\n",
        "X_train_per, X_test_per, y_train_per, y_test_per = train_test_split(total_per, y_per, test_size=0.2, random_state=42)\n",
        "X_train_per.reset_index(drop=True, inplace=True)\n",
        "X_test_per.reset_index(drop=True, inplace=True)\n",
        "y_train_rej.reset_index(drop=True, inplace=True)\n",
        "y_test_rej.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "ijjRBZFHsvLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiiKkafuJbGK"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate, ShuffleSplit, KFold\n",
        "\n",
        "categorical_features_per = categorical_features.copy()\n",
        "numeric_features_per = numeric_features.copy()\n",
        "\n",
        "en=ce.backward_difference.BackwardDifferenceEncoder(cols = categorical_features)\n",
        "\n",
        "train_data_rej, val_data_rej = data_split(X_train_rej, 10, 0.2, 10)\n",
        "train_data_rej.reset_index(drop=True, inplace=True)\n",
        "val_data_rej.reset_index(drop=True, inplace=True)\n",
        "\n",
        "fp_rej = morgan_fp(0, 2048)\n",
        "kf = KFold(shuffle=True, random_state=10)\n",
        "sc_rej = PowerTransformer()\n",
        "\n",
        "y_train_rej.reset_index(drop=True, inplace=True)\n",
        "y_test_rej.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "from rdkit import DataStructs, Chem\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "params_rej = {'fp_length': 2048,\n",
        "              'gamma': 1.5412511395696913,\n",
        "              'learning_rate': 0.08103909978496358,\n",
        "              'max_delta_step': 6.29569181619016,\n",
        "              'max_depth': 6.0,\n",
        "              'min_child_weight': 7.212247399661489,\n",
        "              'n_estimators': 727.0,\n",
        "              'reg_alpha': 3.816910539411469,\n",
        "              'reg_lambda': 4.238030816945888,\n",
        "              'subsample': 0.6034272913431271}\n",
        "\n",
        "fp_rej = morgan_fp(0, params_rej['fp_length'])\n",
        "\n",
        "en_rej=ce.backward_difference.BackwardDifferenceEncoder(cols = categorical_features)\n",
        "\n",
        "\n",
        "x_train_rej = conv_data_pd(X_train_rej, X_train_rej, fp_rej, en_rej, sc_rej, numeric_features, 'rej')\n",
        "x_test_rej = conv_data_pd(X_test_rej, X_train_rej, fp_rej, en_rej, sc_rej, numeric_features, 'rej')\n",
        "\n",
        "model_rej = XGBRegressor(max_delta_step=params_rej['max_delta_step'], learning_rate = params_rej['learning_rate'],\n",
        "                    max_depth = int(params_rej['max_depth']), min_child_weight= params_rej['min_child_weight'],\n",
        "                    subsample=params_rej['subsample'],reg_alpha=params_rej['reg_alpha'],gamma= params_rej['gamma'],\n",
        "                    reg_lambda= params_rej['reg_lambda'],n_estimators=int(params_rej['n_estimators']), random_state=10)\n",
        "model_rej.fit(x_train_rej, y_train_rej)\n",
        "\n",
        "y_train_rej.reset_index(drop=True, inplace=True)\n",
        "y_test_rej.reset_index(drop=True, inplace=True)\n",
        "\n",
        "np.round(r2_score(y_train_rej, model_rej.predict(x_train_rej)), 2)"
      ],
      "metadata": {
        "id": "o8twqikbaoew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30c7ce18-abc1-4a7d-934f-ede224575cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.96"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NinYvUCgIgM"
      },
      "outputs": [],
      "source": [
        "train_data_per, val_data_per = data_split(X_train_per, 10, 0.2, 10)\n",
        "train_data_per.reset_index(drop=True, inplace=True)\n",
        "val_data_per.reset_index(drop=True, inplace=True)\n",
        "\n",
        "fp_per = morgan_fp(0, 2048)\n",
        "kf = KFold(shuffle=True, random_state=10)\n",
        "sc_per = PowerTransformer()\n",
        "\n",
        "y_train_per.reset_index(drop=True, inplace=True)\n",
        "y_test_per.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_per = {'fp_length': 2048,\n",
        "              'gamma': 0.5370309939072397,\n",
        "              'learning_rate': 0.05144141844082313,\n",
        "              'max_delta_step': 6.427683378298868,\n",
        "              'max_depth': 10.0,\n",
        "              'min_child_weight': 9.384698920602181,\n",
        "              'n_estimators': 523.0,\n",
        "              'reg_alpha': 6.7864101789423055,\n",
        "              'reg_lambda': 3.1001740844985735,\n",
        "              'subsample': 0.6699461278318496}\n",
        "\n",
        "categorical_features = ['Organic solvent type', 'Additive X1 in aqueous']\n",
        "\n",
        "numeric_features = ['A1/A2 ratio', 'Monomer A concentration', 'Monomer B concentration', 'Additive X1 concentration',\n",
        "                    'Additive Y concentration', 'Heat curing time', 'Heat curing termperature',\n",
        "                    'salt_concentration', 'cat_val', 'cat_r', 'cat_hyd_r', 'cat_hyd_e', 'an_val', 'an_r', 'an_hyd_r',\n",
        "                    'an_hyd_e', 'Polymerization time']\n",
        "\n",
        "fp_per = morgan_fp(0, params_per['fp_length'])\n",
        "# sc_per = PowerTransformer()\n",
        "en_per=ce.backward_difference.BackwardDifferenceEncoder(cols = categorical_features)\n",
        "\n",
        "x_train_per = conv_data_pd(X_train_per, X_train_per, fp_per, en_per, sc_per, numeric_features, 'per')\n",
        "model_per = XGBRegressor(max_delta_step=params_per['max_delta_step'], learning_rate = params_per['learning_rate'],\n",
        "                    max_depth = int(params_per['max_depth']), min_child_weight= params_per['min_child_weight'],\n",
        "                    subsample=params_per['subsample'],reg_alpha=params_per['reg_alpha'],gamma= params_per['gamma'],\n",
        "                    reg_lambda= params_per['reg_lambda'],n_estimators=int(params_per['n_estimators']), random_state=10)\n",
        "model_per.fit(x_train_per, y_train_per)\n",
        "\n",
        "y_train_per_pred = model_per.predict(x_train_per)\n",
        "\n",
        "np.round(r2_score(y_train_per, y_train_per_pred), 2)"
      ],
      "metadata": {
        "id": "Zi9_sqrMjVGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ac52a0-7d7c-473a-aa69-0cdaa0549351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.82"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_directory = ''"
      ],
      "metadata": {
        "id": "KXgAip9RAq6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SENhNAK5KdcB"
      },
      "outputs": [],
      "source": [
        "mom_l = pd.read_csv(file_directory + 'mom_limit_rej_per_0521.csv')\n",
        "mom_l.drop(columns=['sim'], inplace=True)\n",
        "\n",
        "mom_l.loc[79, 'a-smile']='C'\n",
        "mom_l.loc[79, 'a-mw']=0\n",
        "mom_l = mom_l.rename(columns={'Monomer name': 'Monomer A1 type'})\n",
        "mom_l.loc[79, 'Monomer A1 type']='Carbon'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThAXqLu1Kdp_"
      },
      "outputs": [],
      "source": [
        "MB = df[['Monomer B type', 'b-smile']].drop_duplicates().reset_index(drop=True)\n",
        "MB = MB[MB['b-smile']!='xxx']\n",
        "MB.dropna(axis=0, inplace=True)\n",
        "\n",
        "OB = df[['Organic solvent type']].drop_duplicates().reset_index(drop=True)\n",
        "X1T = df[['Additive X1 in aqueous']].drop_duplicates().reset_index(drop=True)\n",
        "# X2T = df[['Additive X2 type in aqueous phase']].drop_duplicates().reset_index(drop=True)\n",
        "YT = df[['Additive Y type in organic  phase']].drop_duplicates().reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajD-ODHiSl6K"
      },
      "outputs": [],
      "source": [
        "###optimization\n",
        "\n",
        "col = list(train_data_rej.columns)\n",
        "# col.append('permeabiility')\n",
        "\n",
        "space_opt = {'a1-smile': hp.randint('a1-smile', len(mom_l)),\n",
        "             'a2-smile': hp.randint('a2-smile', len(mom_l)),\n",
        "              # 'Monomer B type': hp.randint('Monomer B type', len(MB)),\n",
        "             'b-smile': hp.randint('b-smile', len(MB)),\n",
        "             'Organic solvent type': hp.randint('Organic solvent type', len(OB)),\n",
        "             'Additive X1 in aqueous': hp.randint('Additive X1 in aqueous', len(X1T)),\n",
        "             'Additive Y type in organic  phase': hp.randint('Additive Y type in organic  phase', len(YT)),\n",
        "            #  'Additive X2 type in aqueous phase': hp.randint(' Additive X2 type in aqueous phase', len(X2T)),\n",
        "             'A1/A2 ratio': hp.uniform('A1/A2 ratio', 0, 10),\n",
        "             'Monomer A concentration': hp.uniform('Monomer A concentration', 0.01, 3),\n",
        "             'Monomer B concentration': hp.uniform('Monomer B concentration', 0.01, 1),\n",
        "\n",
        "             'Additive X1 concentration':hp.choice('Additive X1 concentration', [hp.uniform('Additive X1 concentration range', 0, 1), np.nan]),\n",
        "            #  'Additive X2 concentration':hp.choice('Additive X2 concentration', [hp.uniform('Additive X2 concentration range', 0, 1.5), np.nan]),\n",
        "             'Additive Y concentration':hp.choice('Additive Y concentration', [hp.uniform('Additive Y concentration range', 0, 1), np.nan]),\n",
        "\n",
        "            #  'Aqueous phase pH':hp.choice('Aqueous phase pH', [hp.uniform('pH range', 7, 12), np.nan]),\n",
        "             'Polymerization time':hp.quniform('Polymerization time_a', 5, 600, 1),\n",
        "             'Heat curing time':hp.quniform('Heat curing time', 1, 20,1),\n",
        "             'Heat curing temperature': hp.quniform('Heat curing temperature', 22, 70,1),\n",
        "            #  'Transmembrane pressure':hp.quniform('Transmembrane pressure', 2, 10,1)\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPqIfSprSx_3"
      },
      "outputs": [],
      "source": [
        "##Na2SO4\n",
        "\n",
        "def get_x_l(params):\n",
        "    hh = pd.DataFrame(columns=col)\n",
        "    hh.loc[0, 'Monomer A1 type']=mom_l.loc[params['a1-smile'],'Monomer A1 type']\n",
        "    hh.loc[0, 'Monomer A2 type']=mom_l.loc[params['a2-smile'],'Monomer A1 type']\n",
        "    hh.loc[0, 'a1-smile']=mom_l.loc[params['a1-smile'],'a-smile']\n",
        "    hh.loc[0, 'a2-smile']=mom_l.loc[params['a2-smile'],'a-smile']\n",
        "    hh.loc[0, 'b-smile']=MB.loc[params['b-smile'],'b-smile']\n",
        "    hh.loc[0, 'A1/A2 ratio']=params['A1/A2 ratio']\n",
        "    hh.loc[0, 'Monomer A concentration']=params['Monomer A concentration']\n",
        "    hh.loc[0, 'Monomer B concentration']=params['Monomer B concentration']\n",
        "\n",
        "    hh.loc[0, 'Additive Y type in organic  phase']=params['Additive Y type in organic  phase']\n",
        "    # hh.loc[0, 'Additive X2 type in aqueous phase']=params['Additive X2 type in aqueous phase']\n",
        "\n",
        "    hh.loc[0, 'Additive X1 concentration']=params['Additive X1 concentration']\n",
        "    # hh.loc[0, 'Additive X2 concentration']=params['Additive X2 concentration']\n",
        "    hh.loc[0, 'Additive Y concentration']=params['Additive Y concentration']\n",
        "\n",
        "    # hh.loc[0, 'Aqueous phase pH']=params['Aqueous phase pH']\n",
        "    hh.loc[0, 'Polymerization time']=params['Polymerization time']\n",
        "    hh.loc[0, 'Heat curing time']=params['Heat curing time']\n",
        "    hh.loc[0, 'Heat curing temperature']=params['Heat curing temperature']\n",
        "    # hh.loc[0, 'Transmembrane pressure']=params['Transmembrane pressure']\n",
        "\n",
        "    hh.loc[0, 'Organic solvent type']= OB.loc[params['Organic solvent type'], 'Organic solvent type']\n",
        "    hh.loc[0, 'Additive X1 in aqueous']= X1T.loc[params['Additive X1 in aqueous'], 'Additive X1 in aqueous']\n",
        "\n",
        "    hh.loc[0, 'cat_val']=1\n",
        "    hh.loc[0, 'cat_r']=0.102\n",
        "    hh.loc[0, 'cat_hyd_r' ]=0.358\n",
        "    hh.loc[0, 'cat_hyd_e' ]=-365\n",
        "\n",
        "    hh.loc[0, 'an_val']=-2\n",
        "    hh.loc[0, 'an_r']=0.23\n",
        "    hh.loc[0, 'an_hyd_r' ]=0.379\n",
        "    hh.loc[0, 'an_hyd_e' ]=-1080\n",
        "    hh.loc[0, 'salt_concentration']=1000\n",
        "    # hh.loc[0, 'salt_rejection']=np.nan\n",
        "    # hh.loc[0, 'Permeability']=np.nan\n",
        "\n",
        "    return hh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EuJ0Q8eS1Gr"
      },
      "outputs": [],
      "source": [
        "from hyperopt.pyll.stochastic import sample\n",
        "hh = sample(space_opt)\n",
        "d = get_x_l(hh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fAK5OStSyhV"
      },
      "outputs": [],
      "source": [
        "target_perm = 10\n",
        "\n",
        "def fit(params):\n",
        "    hh = get_x_l(params)\n",
        "    if hh['a1-smile'][0] == hh['a2-smile'][0]:\n",
        "        hh.loc[0, 'a2-smile'] = 'C'\n",
        "        hh.loc[0, 'a2-mw'] = 0\n",
        "    for i in hh[hh['Additive X1 in aqueous'].isnull() == True].index:\n",
        "        hh.loc[i, 'Additive X1 concentration'] = 0\n",
        "    if hh['b-smile'][0] == 'C':\n",
        "        hh.loc[0, 'A1/A2 ratio'] = 0\n",
        "\n",
        "    x_salt = conv_data_pd_opt(hh, X_train_rej, fp_rej, en_rej, sc_rej, numeric_features, 'rej')\n",
        "    x_per = conv_data_pd_opt(hh, X_train_per, fp_per, en_per, sc_per, numeric_features, 'per')\n",
        "\n",
        "    # Ensure numeric features are of type float\n",
        "    x_salt[numeric_features] = x_salt[numeric_features].astype(float)\n",
        "    x_per[numeric_features] = x_per[numeric_features].astype(float)\n",
        "\n",
        "    x_salt.to_csv('x_salt.csv')\n",
        "\n",
        "    pred = model_rej.predict(x_salt)[0]\n",
        "    per_pred = model_per.predict(x_per)[0]\n",
        "\n",
        "    return abs((100 - pred)/per_pred) + abs(per_pred - target_perm), (100 - pred)/per_pred, pred, per_pred\n",
        "\n",
        "\n",
        "def objective(params):\n",
        "    global ITERATION\n",
        "    ITERATION +=1\n",
        "\n",
        "    loss,y, salt, per = fit(params)\n",
        "\n",
        "    off_connection = open(out_file, 'a')\n",
        "    writer = csv.writer(off_connection)\n",
        "    writer.writerow([loss,y, salt, per, params, ITERATION])\n",
        "    return {'loss':loss,'Y':y,'salt':salt, 'per':per, 'params': params, 'iteration':ITERATION, 'status':STATUS_OK}\n",
        "\n",
        "import csv\n",
        "out_file = '/gdrive/My Drive/ML-BO_followup/' + 'salt_opt_l10_withnm.csv'\n",
        "off_connection =open(out_file, 'w')\n",
        "writer = csv.writer(off_connection)\n",
        "writer.writerow(['loss','Y','salt', 'per', 'params', 'iteration'])\n",
        "off_connection.close()\n",
        "\n",
        "tpe_algo = tpe.suggest\n",
        "bayes_trial = Trials()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btgFZXU6TCCF",
        "outputId": "e00438dc-61ba-4ade-a2a3-a2e6150a427a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 37%|███▋      | 3704/10000 [1:31:19<2:49:10,  1.61s/trial, best loss: 0.020843031786254976]"
          ]
        }
      ],
      "source": [
        "#%%capture\n",
        "global ITERATION\n",
        "ITERATION = 0\n",
        "best = fmin(fn = objective, space=space_opt, algo = tpe_algo, trials = bayes_trial, max_evals=10000, rstate=np.random.default_rng(np.random.randint(0,50)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDcmaISeYH8Y6sUKqHu7wE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}