{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nohyeong/ML-BO_membrane_design/blob/main/Li_Bayesian_final_per_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwWJFB4u7_kx",
        "outputId": "bfa74e87-b3fa-4136-d9ca-ef8fdb4c0af5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2023.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2023.9.6\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.11.4)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (2.0.3)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.5.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.0)\n",
            "Installing collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.6.3\n",
            "Collecting shap\n",
            "  Downloading shap-0.45.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.5/540.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.0.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.4)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.0)\n",
            "Collecting slicer==0.0.8 (from shap)\n",
            "  Downloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.45.1 slicer-0.0.8\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl (98.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.3.0)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2.5\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "!pip install rdkit\n",
        "!pip install category_encoders\n",
        "!pip install shap\n",
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3FRknVV8E6k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import ast\n",
        "import pytz\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from rdkit import Chem\n",
        "from datetime import datetime\n",
        "from rdkit.Chem import AllChem\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import category_encoders as ce\n",
        "from hyperopt import hp, tpe, Trials, STATUS_OK, fmin\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "from rdkit import DataStructs, Chem\n",
        "from rdkit.Chem import AllChem\n",
        "import shap\n",
        "from sklearn.model_selection import cross_validate, ShuffleSplit, KFold\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor,AdaBoostRegressor,BaggingRegressor,ExtraTreesRegressor,GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import Ridge,ARDRegression,BayesianRidge,ElasticNet,GammaRegressor,HuberRegressor\n",
        "from sklearn.linear_model import Lasso, LassoLars, LinearRegression, LogisticRegression, PassiveAggressiveRegressor,Ridge,SGDRegressor\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, PowerTransformer, QuantileTransformer\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.metrics import r2_score\n",
        "from scipy.stats import linregress\n",
        "\n",
        "def make_plot(total_data, range, tick):\n",
        "\n",
        "  X_title = 'Real removal (%)'\n",
        "  Y_title = 'Pred removal (%)'\n",
        "\n",
        "  title_font_size = 43\n",
        "  tickfont_size = 35\n",
        "  ticklen = 5\n",
        "  tickwidth = 2\n",
        "  title_standoff = 20\n",
        "\n",
        "  plot_width = 610\n",
        "  plot_height = 500\n",
        "\n",
        "  fig = px.scatter(total_data, x='y_true', y='y_pred', color = 'mark',\n",
        "                  width=plot_width, height=plot_height, range_x=[-5, range], range_y=[-5, range]\n",
        "                  )\n",
        "\n",
        "  fig.update_layout(\n",
        "      xaxis = dict(\n",
        "          dtick=tick,\n",
        "          title_text = X_title,\n",
        "          title_font = {\"size\": title_font_size},\n",
        "          tickfont = dict(family='Arial', color='black', size=tickfont_size),\n",
        "          title_standoff = title_standoff,\n",
        "          color='black',\n",
        "          ticks=\"inside\",\n",
        "          ticklen=ticklen,\n",
        "          tickwidth=tickwidth,\n",
        "          title_font_family=\"Arial\"),\n",
        "      yaxis = dict(\n",
        "          dtick = tick,\n",
        "          title_text = Y_title,\n",
        "          title_font = {\"size\": title_font_size},\n",
        "          tickfont = dict(family='Arial', color='black', size=tickfont_size),\n",
        "          title_standoff = title_standoff,\n",
        "          color='black',\n",
        "          ticks=\"inside\",\n",
        "          ticklen=ticklen,\n",
        "          tickwidth=tickwidth,\n",
        "          title_font_family=\"Arial\"))\n",
        "\n",
        "  fig.update_layout(plot_bgcolor='rgb(256,256,256)', showlegend=True)\n",
        "\n",
        "  fig.update_xaxes(showline=True, linewidth=4, linecolor='black', mirror=True)\n",
        "  fig.update_yaxes(showline=True, linewidth=4, linecolor='black', mirror=True)\n",
        "\n",
        "  random_x = [0, 100]\n",
        "  random_y0 = [0, 100]\n",
        "\n",
        "  fig.add_trace(go.Scatter(x=random_x, y=random_y0, line=dict(color='red', width=2), marker=dict(size=1)))\n",
        "\n",
        "  fig.update_traces(marker=dict(size=10, opacity=0.65, line=dict(width=0.7)), selector=dict(mode='markers'))\n",
        "\n",
        "  fig.show()\n",
        "\n",
        "def hyp_opt(objective):\n",
        "\n",
        "  out_file = file_directory + 'per_model_0418.csv'\n",
        "  off_connection =open(out_file, 'w')\n",
        "  writer = csv.writer(off_connection)\n",
        "  writer.writerow(['loss','train_loss', 'params', 'iteration'])\n",
        "  off_connection.close()\n",
        "  tpe_algo = tpe.suggest\n",
        "  bayes_trial = Trials()\n",
        "\n",
        "  global ITERATION\n",
        "  ITERATION = 0\n",
        "\n",
        "  tpe_algo = tpe.suggest\n",
        "  bayes_trial = Trials()\n",
        "\n",
        "  best = fmin(fn=objective, space=space, algo=tpe_algo, trials=bayes_trial, max_evals=50, rstate=np.random.default_rng(np.random.randint(0,50)))\n",
        "  best['fp_length'] = int(fp_length_list[best['fp_length']])\n",
        "\n",
        "  return best\n",
        "\n",
        "def createFolder(directory):\n",
        "    try:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "    except OSError:\n",
        "        print('Error: Creating directory. ' + directory)\n",
        "\n",
        "class morgan_fp:\n",
        "    def __init__(self, radius, length):\n",
        "        self.radius = radius\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, smiles):\n",
        "        if pd.isna(smiles):\n",
        "            smiles = \"\"\n",
        "\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "        if mol:\n",
        "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, self.radius, self.length)\n",
        "            npfp = np.array(list(fp.ToBitString())).astype('float32')\n",
        "        else:\n",
        "            npfp = np.zeros(self.length, dtype='float32')\n",
        "\n",
        "        return npfp\n",
        "\n",
        "def fit(params, train_data, y_train, numeric_features, salt_per):\n",
        "    y_train_clean = y_train.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "    train_data_clean = train_data.loc[y_train_clean.index]\n",
        "\n",
        "    fp = morgan_fp(0, 2048)\n",
        "    model = XGBRegressor(max_delta_step=params['max_delta_step'], learning_rate=params['learning_rate'],\n",
        "                         max_depth=params['max_depth'], min_child_weight=params['min_child_weight'],\n",
        "                         subsample=params['subsample'], reg_alpha=params['reg_alpha'], gamma=params['gamma'],\n",
        "                         reg_lambda=params['reg_lambda'], n_estimators=params['n_estimators'], random_state=10)\n",
        "    kf = KFold(shuffle=True, random_state=10)\n",
        "    t_rmse = []\n",
        "    v_rmse = []\n",
        "\n",
        "    train_data_clean.reset_index(drop=True, inplace=True)\n",
        "    y_train_clean.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    en.fit(train_data_clean[categorical_features])\n",
        "\n",
        "    for train_index, test_index in kf.split(train_data_clean):\n",
        "        x_train = train_data_clean.loc[train_index].reset_index(drop=True)\n",
        "        y_train_split = y_train_clean.loc[train_index].reset_index(drop=True)\n",
        "\n",
        "        x_val = train_data_clean.loc[test_index].reset_index(drop=True)\n",
        "        y_val = y_train_clean.loc[test_index].reset_index(drop=True)\n",
        "\n",
        "        x_train = conv_data_pd(x_train, x_train, fp, en, sc, numeric_features, salt_per)\n",
        "        x_val = conv_data_pd(x_val, x_train, fp, en, sc, numeric_features, salt_per)\n",
        "\n",
        "        model.fit(x_train, y_train_split)\n",
        "        t_rmse.append(np.sqrt(mean_squared_error(y_train_split, model.predict(x_train))))\n",
        "        v_rmse.append(np.sqrt(mean_squared_error(y_val, model.predict(x_val))))\n",
        "\n",
        "    return np.mean(v_rmse), np.mean(t_rmse)\n",
        "\n",
        "def conv_data_pd(data, data_t, fp, en, scaler, numeric_features, salt_per):\n",
        "\n",
        "    data['a1-fp'] = data['a1-smile'].apply(fp)\n",
        "    data['a2-fp'] = data['a2-smile'].apply(fp)\n",
        "    data['a-fp'] = [x + y for x, y in zip(data['a1-fp'], data['a2-fp'])]\n",
        "    data['b-fp'] = data['b-smile'].apply(fp)\n",
        "\n",
        "    x_a = np.array(list(data['a-fp']))\n",
        "    x_b = np.array(list(data['b-fp']))\n",
        "\n",
        "    columns_to_drop = ['Monomer A1 type', 'a1-smile', 'Monomer A2 type', 'a2-smile', 'b-smile', 'a1-fp', 'a2-fp', 'a-fp', 'b-fp']\n",
        "\n",
        "    if salt_per == 'per':\n",
        "        X_train = data_t.drop(columns=columns_to_drop, errors='ignore').copy()\n",
        "        x = data.drop(columns=columns_to_drop, errors='ignore').copy()\n",
        "    elif salt_per == 'rej':\n",
        "        X_train = data_t.drop(columns=columns_to_drop, errors='ignore').copy()\n",
        "        x = data.drop(columns=columns_to_drop, errors='ignore').copy()\n",
        "\n",
        "    col_pd = en.transform(x[categorical_features])\n",
        "    xxxx = x[numeric_features]\n",
        "    num_pd = pd.DataFrame(data=xxxx, columns=numeric_features)\n",
        "    xxxxx = np.concatenate((x_a, x_b), axis=1)\n",
        "    fp_pd = pd.DataFrame(data=xxxxx, columns=[f'f_{i}' for i in range(2 * x_a.shape[1])])\n",
        "\n",
        "    x_pd = pd.concat([fp_pd, col_pd, num_pd], axis=1)\n",
        "    x_pd.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return x_pd\n",
        "\n",
        "def objective_rej(params):\n",
        "    global ITERATION\n",
        "    ITERATION +=1\n",
        "    for name in ['max_depth', 'n_estimators','fp_length']:\n",
        "        params[name] = int(params[name])\n",
        "    loss, train_loss = fit(params, total_rej, y_rej, numeric_features_rej, 'rej')\n",
        "    out_file = 'outfile.csv'\n",
        "    off_connection = open(out_file, 'a')\n",
        "    writer = csv.writer(off_connection)\n",
        "    writer.writerow([loss,train_loss, params, ITERATION])\n",
        "    bayes_trial = Trials()\n",
        "    pickle.dump(bayes_trial, open(\"rej_2_xgb.p\", \"wb\"))\n",
        "    return {'loss':loss,'train_loss':train_loss, 'params': params, 'iteration':ITERATION, 'status':STATUS_OK}\n",
        "\n",
        "def objective_per(params):\n",
        "    global ITERATION\n",
        "    ITERATION +=1\n",
        "    for name in ['max_depth', 'n_estimators','fp_length']:\n",
        "        params[name] = int(params[name])\n",
        "    loss, train_loss = fit(params, total_per, y_per, numeric_features_per, 'per')\n",
        "    out_file = 'outfile.csv'\n",
        "    off_connection = open(out_file, 'a')\n",
        "    writer = csv.writer(off_connection)\n",
        "    writer.writerow([loss,train_loss, params, ITERATION])\n",
        "    bayes_trial = Trials()\n",
        "    pickle.dump(bayes_trial, open(\"per_2_xgb.p\", \"wb\"))\n",
        "    return {'loss':loss,'train_loss':train_loss, 'params': params, 'iteration':ITERATION, 'status':STATUS_OK}\n",
        "\n",
        "\n",
        "def data_split(train_data_per, n_splits, test_size, random_state):\n",
        "  sss = ShuffleSplit(n_splits=1, test_size=0.2,random_state=10)\n",
        "\n",
        "  for train_index, test_index in sss.split(train_data_per):\n",
        "      train_data = train_data_per.iloc[train_index]\n",
        "      test = train_data_per.iloc[test_index]\n",
        "      train_data.reset_index(drop=True, inplace=True)\n",
        "      test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  return train_data, test\n",
        "\n",
        "file_directory = ''\n",
        "sc = StandardScaler()\n",
        "\n",
        "fp_length_list = [2048]\n",
        "\n",
        "space = {'max_delta_step': hp.uniform('max_delta_step', 0, 10),\n",
        "         'learning_rate': hp.uniform('learning_rate', 0.001, 0.1),\n",
        "         'max_depth': hp.quniform('max_depth', 1,10,1),\n",
        "         'min_child_weight': hp.uniform('min_child_weight', 1,10),\n",
        "         'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
        "         'reg_alpha':hp.uniform('reg_alpha', 0, 10),\n",
        "         'gamma':hp.uniform('gamma', 0, 10),\n",
        "         'reg_lambda':hp.uniform('reg_lambda', 0, 10),\n",
        "         'n_estimators': hp.quniform('n_estimators', 1, 1000, 1),\n",
        "         'fp_length': hp.choice('fp_length', fp_length_list)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHfZqYuIyTvt"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# model_rej\n",
        "\n",
        "now_utc = datetime.now(pytz.utc)\n",
        "est_timezone = pytz.timezone('US/Eastern')\n",
        "now_est = now_utc.astimezone(est_timezone)\n",
        "date = now_est.strftime('%Y%m%d_%H:%M:%S')\n",
        "\n",
        "title_rej = 'rej_prediction_XGBoost'\n",
        "\n",
        "directory_rej = '' + date + '_' + title_rej + '/'\n",
        "createFolder(directory_rej)\n",
        "\n",
        "columns_removed = ['Pure_water_flux, LMH/bar', 'DOI', 'cation','Additive X2 type in aqueous phase',\n",
        "                   'Additive X2 concentration, wt%', 'Transmembrane pressure, bar', 'anion', 'Aqueous phase pH']\n",
        "\n",
        "\n",
        "df = pd.read_excel('ML-BO_data_Li.xlsx', sheet_name='Datasets')\n",
        "df.drop(columns_removed, axis=1, inplace=True)\n",
        "\n",
        "df.columns = ['Monomer A1 type', 'a1-smile', 'Monomer A2 type', 'a2-smile',\n",
        "              'A1/A2 ratio', 'Monomer A concentration', 'Monomer B type',\n",
        "              'b-smile', 'Monomer B concentration', 'Organic solvent type',\n",
        "              'Additive X1 in aqueous', 'Additive X1 concentration',\n",
        "               'Additive Y type in organic  phase',\n",
        "              'Additive Y concentration', 'Heat curing time',\n",
        "              'Heat curing termperature',\n",
        "              'salt_concentration', 'cat_val', 'cat_r', 'cat_hyd_r', 'cat_hyd_e',\n",
        "              'an_val', 'an_r', 'an_hyd_r', 'an_hyd_e', 'Polymerization time',\n",
        "              'Permeability', 'salt_rejection', 'Done']\n",
        "\n",
        "df['Monomer A concentration'] = pd.to_numeric(df['Monomer A concentration'], errors='coerce')\n",
        "df['salt_rejection'] = pd.to_numeric(df['salt_rejection'], errors='coerce')\n",
        "\n",
        "categorical_features = ['Organic solvent type', 'Additive X1 in aqueous']\n",
        "\n",
        "numeric_features_rej = ['A1/A2 ratio', 'Monomer A concentration', 'Monomer B concentration', 'Additive X1 concentration',\n",
        "                        'Additive Y concentration', 'Heat curing time', 'Heat curing termperature',\n",
        "                        'salt_concentration', 'cat_val', 'cat_r', 'cat_hyd_r', 'cat_hyd_e', 'an_val', 'an_r', 'an_hyd_r',\n",
        "                        'an_hyd_e', 'Polymerization time']\n",
        "\n",
        "numeric_features_per = ['A1/A2 ratio', 'Monomer A concentration', 'Monomer B concentration', 'Additive X1 concentration',\n",
        "                        'Additive Y concentration', 'Heat curing time', 'Heat curing termperature',\n",
        "                        'salt_concentration', 'Polymerization time']\n",
        "\n",
        "df[numeric_features_rej] = df[numeric_features_rej].replace('-', np.nan)\n",
        "\n",
        "df = df[df['a1-smile'] != 'xxx']\n",
        "df = df[df['a2-smile'] != 'xxx']\n",
        "df = df[df['b-smile'] != 'xxx']\n",
        "df = df[df['Monomer A1 type'] != 'Potassium 2,5-dihydroxybenzenesulfonate']\n",
        "\n",
        "df['a1-smile'] = df['a1-smile'].fillna('C')\n",
        "df['a2-smile'] = df['a2-smile'].fillna('C')\n",
        "df['b-smile'] = df['b-smile'].fillna('C')\n",
        "\n",
        "total_df = df[df['Done']==1]\n",
        "total_df.reset_index(drop=True, inplace=True)\n",
        "total_df.drop(columns=['Done', 'Monomer B type'], inplace=True)\n",
        "\n",
        "total_df['salt_rejection'] = total_df['salt_rejection'].replace('-', np.nan)\n",
        "total_df['Permeability'] = total_df['Permeability'].replace('-', np.nan)\n",
        "total_df['a2-smile'] = total_df['a2-smile'].replace('-', np.nan)\n",
        "\n",
        "total_rej = total_df.dropna(subset=['salt_rejection'])\n",
        "y_rej = total_rej['salt_rejection']\n",
        "total_rej = total_rej.drop(columns=['Permeability', 'salt_rejection'])\n",
        "\n",
        "X_train_rej, X_test_rej, y_train_rej, y_test_rej = train_test_split(total_rej, y_rej, test_size=0.2, random_state=42)\n",
        "X_train_rej.reset_index(drop=True, inplace=True)\n",
        "X_test_rej.reset_index(drop=True, inplace=True)\n",
        "\n",
        "total_per = total_df.dropna(subset=['Permeability'])\n",
        "y_per = total_per['Permeability']\n",
        "total_per = total_per.drop(columns=['Permeability', 'salt_rejection'])\n",
        "\n",
        "X_train_per, X_test_per, y_train_per, y_test_per = train_test_split(total_per, y_per, test_size=0.2, random_state=42)\n",
        "X_train_per.reset_index(drop=True, inplace=True)\n",
        "X_test_per.reset_index(drop=True, inplace=True)\n",
        "\n",
        "en=ce.backward_difference.BackwardDifferenceEncoder(cols = categorical_features)\n",
        "en_per=ce.backward_difference.BackwardDifferenceEncoder(cols = categorical_features)\n",
        "\n",
        "train_data_rej, val_data_rej = data_split(X_train_rej, 10, 0.2, 10)\n",
        "train_data_rej.reset_index(drop=True, inplace=True)\n",
        "val_data_rej.reset_index(drop=True, inplace=True)\n",
        "\n",
        "fp_rej = morgan_fp(0, 2048)\n",
        "kf = KFold(shuffle=True, random_state=10)\n",
        "sc_rej = PowerTransformer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_per = hyp_opt(objective_per)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7byPSwODwbIe",
        "outputId": "67923944-eeed-484f-add0-7468edc52355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 50/50 [59:57<00:00, 71.95s/trial, best loss: 2.0096543557769437]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_per.reset_index(drop=True, inplace=True)\n",
        "y_test_per.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "zb_7lSuTxxAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp = morgan_fp(0, params_per['fp_length'])\n",
        "sc = PowerTransformer()\n",
        "\n",
        "x_train_per = conv_data_pd(X_train_per, X_train_per, fp, en, sc, numeric_features_per, 'per')\n",
        "model_per = XGBRegressor(max_delta_step=params_per['max_delta_step'], learning_rate = params_per['learning_rate'],\n",
        "                    max_depth = int(params_per['max_depth']), min_child_weight= params_per['min_child_weight'],\n",
        "                    subsample=params_per['subsample'],reg_alpha=params_per['reg_alpha'],gamma= params_per['gamma'],\n",
        "                    reg_lambda= params_per['reg_lambda'],n_estimators=int(params_per['n_estimators']), random_state=10)\n",
        "model_per.fit(x_train_per, y_train_per)\n",
        "\n",
        "x_test_per = conv_data_pd(X_test_per, X_train_per, fp, en, sc, numeric_features_per, 'per')\n",
        "\n",
        "y_train_per_pred = model_per.predict(x_train_per)\n",
        "y_test_per_pred = model_per.predict(x_test_per)\n",
        "\n",
        "x_train_per['y_true'] = y_train_per\n",
        "x_test_per['y_true'] = y_test_per\n",
        "x_train_per['y_pred'] = y_train_per_pred\n",
        "x_test_per['y_pred'] = y_test_per_pred\n",
        "\n",
        "x_train_per['mark'] = 'train'\n",
        "x_test_per['mark'] = 'test'\n",
        "total_data_per = pd.concat([x_train_per, x_test_per])\n",
        "\n",
        "\n",
        "train_pred_mae = mean_absolute_error(y_train_per, y_train_per_pred)\n",
        "train_pred_rmse = mean_squared_error(y_train_per, y_train_per_pred, squared = False)\n",
        "\n",
        "pred_mae = mean_absolute_error(y_test_per_pred, y_test_per)\n",
        "pred_rmse = mean_squared_error(y_test_per_pred, y_test_per, squared = False)\n",
        "\n",
        "print(F\"MAE: {np.round(pred_mae,2)}, RMSE: {np.round(pred_rmse,2)}\")\n",
        "print(F\"Train MAE: {np.round(train_pred_mae,2)}, Train RMSE: {np.round(train_pred_rmse,2)}\")\n",
        "\n",
        "r2 = r2_score(y_test_per_pred, y_test_per)\n",
        "print(\"R2: \", np.round(r2,2))\n",
        "make_plot(total_data_per, 35, 10)"
      ],
      "metadata": {
        "id": "yhMSTPw7-6N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naYALRuqd17I"
      },
      "outputs": [],
      "source": [
        "x_train_new = x_train_per.drop(['y_true', 'y_pred', 'mark'], axis=1)\n",
        "\n",
        "explainer = shap.TreeExplainer(model_per)\n",
        "shap_value_train_per = explainer.shap_values(x_train_new)\n",
        "#shap_value_train = explainer.shap_values(x_test)\n",
        "shap.summary_plot(shap_value_train_per, x_train_new, max_display=20, show=False)\n",
        "plt.xlabel('SHAP value', fontsize=18,weight='bold')\n",
        "plt.xticks(fontsize=18, weight='bold')\n",
        "plt.yticks(fontsize=18, weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('per_20.png', dpi = 400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2tkoTiC1kuC"
      },
      "outputs": [],
      "source": [
        "def get_positive_fp(X_train, shap_value, fp_length):\n",
        "\n",
        "  positive_fp_per=[]\n",
        "  for i in range(fp_length):\n",
        "      hh=[]\n",
        "      for j in X_train[f'f_{i}'][X_train[f'f_{i}']==1].index:\n",
        "          hh.append(shap_value[j, i])\n",
        "      if np.sum(hh)>0:\n",
        "          positive_fp_per.append(i)\n",
        "\n",
        "  return positive_fp_per\n",
        "\n",
        "positive_fp_per = get_positive_fp(x_train_per_new, shap_value_train_per, 1024)\n",
        "positive_fp_per"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5toViJ2PikVL"
      },
      "outputs": [],
      "source": [
        "positive_fp_rej = get_positive_fp(x_train_new, shap_value_train_rej, 2048)\n",
        "positive_fp_rej"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLUREWPWOhyo"
      },
      "outputs": [],
      "source": [
        "positive_fp = positive_fp_rej + positive_fp_per"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcQB3K3W-W06"
      },
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_value_train_rej[:, positive_fp], x_train_new[[f'f_{i}' for i in positive_fp]],max_display=20, show=False)\n",
        "plt.xlabel('SHAP value', fontsize=18,weight='bold')\n",
        "plt.xticks(fontsize=18, weight='bold')\n",
        "plt.yticks(fontsize=18, weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('per_20_fp.png', dpi =400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuQdC4J9zd2l"
      },
      "outputs": [],
      "source": [
        "items_to_remove = [1, 1114, 926, 926, 650, 114, 695, 881]\n",
        "# items_to_remove = [623, 128, 1104, 695, 114]\n",
        "for item in items_to_remove:\n",
        "    if item in positive_fp:\n",
        "        positive_fp.remove(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOlp_PyQ7c-E"
      },
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_value_train_rej[:, positive_fp], x_train_new[[f'f_{i}' for i in positive_fp]],max_display=20, show=False)\n",
        "plt.xlabel('SHAP value', fontsize=18,weight='bold')\n",
        "plt.xticks(fontsize=18, weight='bold')\n",
        "plt.yticks(fontsize=18, weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('per_20_fp.png', dpi =400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-LuIFDtAtie"
      },
      "outputs": [],
      "source": [
        "positive_fp_rej_1=[]\n",
        "for i in range(int(params_rej['fp_length']), 2*int(params_rej['fp_length'])):\n",
        "    hh=[]\n",
        "    for j in x_train_rej[f'f_{i}'][x_train_rej[f'f_{i}']==1].index:\n",
        "        hh.append(shap_value_train_rej[j, i])\n",
        "    if np.sum(hh)>0:\n",
        "        positive_fp_rej_1.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3KdIagyv5J2"
      },
      "outputs": [],
      "source": [
        "positive_fp_per_1=[]\n",
        "for i in range(int(params_per['fp_length']), 2*int(params_per['fp_length'])):\n",
        "    hh=[]\n",
        "    for j in x_train_per[f'f_{i}'][x_train_per[f'f_{i}']==1].index:\n",
        "        hh.append(shap_value_train_per[j, i])\n",
        "    if np.sum(hh)>0:\n",
        "        positive_fp_per_1.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTgUOu43v5S8"
      },
      "outputs": [],
      "source": [
        "def ta_index(a, b):\n",
        "    a_bits = np.sum(a)\n",
        "    b_bits = np.sum(b)\n",
        "    h=0\n",
        "    for i in range(len(a)):\n",
        "        if a[i]==1 and b[i]==1:\n",
        "            h+=1\n",
        "    c_bits=h\n",
        "    return c_bits/(a_bits+b_bits-c_bits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mT6tHHrUv5YO"
      },
      "outputs": [],
      "source": [
        "mom = pd.read_excel('fina_data 041621.xlsx', sheet_name = 'monomer database')\n",
        "mom = mom[['Monomer name','SMILES', 'Molecular Weight']]\n",
        "mom.columns=['Monomer name','a-smile', 'a-mw']\n",
        "mom.dropna(inplace=True)\n",
        "mom.reset_index(drop=True, inplace=True)\n",
        "len(mom)\n",
        "mom.loc[301,'a-smile']='C'\n",
        "mom.loc[301,'a-mw']=0\n",
        "mom_per = mom.copy()\n",
        "mom_rej = mom.copy()\n",
        "mom_total = mom.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO8c54Q3v5cI"
      },
      "outputs": [],
      "source": [
        "data = pd.read_excel('data_per_0416.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrJ9-ezVv5hX",
        "outputId": "bc83aadd-bd28-440c-af7f-9d78b4a5a213"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.0"
            ]
          },
          "execution_count": 396,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pfp = np.zeros(2048)\n",
        "for i in positive_fp_per:\n",
        "    pfp[i]=1\n",
        "np.sum(pfp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds15w7SZv5mV",
        "outputId": "51632ac0-cd20-44d6-8aa6-45b2edbf3791"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7.0"
            ]
          },
          "execution_count": 397,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rfp = np.zeros(2048)\n",
        "for i in positive_fp_rej:\n",
        "    rfp[i]=1\n",
        "np.sum(rfp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go88csW-v5qv"
      },
      "outputs": [],
      "source": [
        "total_fp = rfp + pfp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtLGbOPov5v2",
        "outputId": "82948cac-b907-4129-d10a-584aa822db08"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[10:46:44] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 4 19 20 21 22\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(mom_total)):\n",
        "    smiles = mom['a-smile'][i]\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is not None:  # Check if mol is valid\n",
        "        fp = fp_rej(smiles)\n",
        "        mom_total.loc[i, 'sim'] = ta_index(fp, total_fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fne191yjAtsT"
      },
      "outputs": [],
      "source": [
        "mom_total.sort_values('sim', ascending= False, inplace = True)\n",
        "mom_total.reset_index(drop = True, inplace =True)\n",
        "# mom_total.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYNm-MzDxK61"
      },
      "outputs": [],
      "source": [
        "mom_l = mom_total[mom_total['sim']>0.1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZWAAhumxLAn"
      },
      "outputs": [],
      "source": [
        "mom_l.drop_duplicates(keep='first', inplace=True)\n",
        "mom_l.reset_index(drop=True, inplace=True)\n",
        "mom_l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbjJV7dlxLKn"
      },
      "outputs": [],
      "source": [
        "mom_l.to_csv(file_directory + 'mom_limit_rej_per_0418.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMMWuVzgoOLYJcQZUo9aC4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}